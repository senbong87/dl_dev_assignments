{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, BatchNormalization, \\\n",
    "                         Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, \\\n",
    "                            ReduceLROnPlateau\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_map = {}\n",
    "max_class = 100\n",
    "for i,f in enumerate(glob.glob(\"data/*.npy\")):\n",
    "    if i < max_class:\n",
    "        data_map[os.path.basename(f)] = np.load(f)\n",
    "\n",
    "train_samples_per_class = 2**10\n",
    "test_samples_per_class = 2**8\n",
    "train_data_map, test_data_map = {}, {}\n",
    "for key in data_map:\n",
    "    train_data_map[key] = data_map[key].reshape((-1, 28, 28))[:,:,:,np.newaxis][:train_samples_per_class]\n",
    "    test_data_map[key] = data_map[key].reshape((-1, 28, 28))[:,:,:,np.newaxis][train_samples_per_class:train_samples_per_class+test_samples_per_class]\n",
    "    train_data_map[key] = train_data_map[key].astype(\"float32\") / 255\n",
    "    test_data_map[key] = test_data_map[key].astype(\"float32\") / 255\n",
    "\n",
    "class_orders = data_map.keys()\n",
    "train_data = np.concatenate([train_data_map[key] for key in class_orders])\n",
    "test_data = np.concatenate([test_data_map[key] for key in class_orders])\n",
    "train_target = np.concatenate([train_data_map[k].shape[0]*[i]\n",
    "                               for i,k in enumerate(class_orders)])\n",
    "test_target = np.concatenate([test_data_map[k].shape[0]*[i]\n",
    "                              for i,k in enumerate(class_orders)])\n",
    "train_target = np_utils.to_categorical(train_target)\n",
    "test_target = np_utils.to_categorical(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leg.npy', 'keyboard.npy', 'mermaid.npy', 'strawberry.npy', 'dolphin.npy', 'nail.npy', 'vase.npy', 'bus.npy', 'trumpet.npy', 'elephant.npy', 'television.npy', 'bracelet.npy', 'basketball.npy', 't-shirt.npy', 'banana.npy', 'sleeping bag.npy', 'eraser.npy', 'lion.npy', 'hourglass.npy', 'tennis racquet.npy', 'chandelier.npy', 'airplane.npy', 'shovel.npy', 'hamburger.npy', 'matches.npy', 'pool.npy', 'broccoli.npy', 'drill.npy', 'diamond.npy', 'guitar.npy', 'hat.npy', 'duck.npy', 'lighter.npy', 'book.npy', 'piano.npy', 'toothpaste.npy', 'mountain.npy', 'car.npy', 'postcard.npy', 'tractor.npy', 'shorts.npy', 'crocodile.npy', 'octagon.npy', 'beard.npy', 'drums.npy', 'clarinet.npy', 'oven.npy', 'The Great Wall of China.npy', 'bat.npy', 'hot air balloon.npy', 'anvil.npy', 'flashlight.npy', 'bench.npy', 'boomerang.npy', 'spider.npy', 'hot dog.npy', 'sweater.npy', 'barn.npy', 'computer.npy', 'bird.npy', 'sailboat.npy', 'bridge.npy', 'hockey stick.npy', 'roller coaster.npy', 'wine bottle.npy', 'fan.npy', 'wristwatch.npy', 'tooth.npy', 'traffic light.npy', 'remote control.npy', 'horse.npy', 'van.npy', 'pizza.npy', 'foot.npy', 'picture frame.npy', 'camel.npy', 'suitcase.npy', 'sock.npy', 'skull.npy', 'flamingo.npy', 'donut.npy', 'hexagon.npy', 'leaf.npy', 'pillow.npy', 'necklace.npy', 'jail.npy', 'hand.npy', 'grass.npy', 'star.npy', 'cloud.npy', 'feather.npy', 'snowman.npy', 'fire hydrant.npy', 'baseball bat.npy', 'knife.npy', 'screwdriver.npy', 'house.npy', 'compass.npy', 'moon.npy', 'rabbit.npy']\n",
      "(102400, 28, 28, 1) (102400, 100) (25600, 28, 28, 1) (25600, 100)\n"
     ]
    }
   ],
   "source": [
    "print data_map.keys()\n",
    "print train_data.shape, train_target.shape, test_data.shape, test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_class = len(data_map)\n",
    "\n",
    "train_data_gen = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
    "                                    height_shift_range=0.1, horizontal_flip=True,\n",
    "                                    vertical_flip=False)\n",
    "test_data_gen = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1, horizontal_flip=True,\n",
    "                                   vertical_flip=False)\n",
    "train_data_gen.fit(train_data)\n",
    "test_data_gen.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(28, 28, 1), name='Input_layer')\n",
    "\n",
    "# convblock 01\n",
    "x = Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "           input_shape=input_layer.shape, name='Conv01_layer')(input_layer)\n",
    "x = Dropout(0.05, name='Dropout06')(x)\n",
    "x = BatchNormalization(name='BatchNormalization01_layer')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', name='Conv02_layer')(x)\n",
    "x = Dropout(0.05, name='Dropout05')(x)\n",
    "x = BatchNormalization(name='BatchNormalization02_layer')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', name='Conv03_layer')(x)\n",
    "x = Dropout(0.05, name='Dropout04')(x)\n",
    "x = BatchNormalization(name='BatchNormalization03_layer')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), name='MaxPool01_layer')(x)\n",
    "\n",
    "# convblock 02\n",
    "x = Conv2D(64, (3, 3), padding='same', activation='relu',\n",
    "           input_shape=input_layer.shape, name='Conv04_layer')(x)\n",
    "x = Dropout(0.05, name='Dropout03')(x)\n",
    "x = BatchNormalization(name='BatchNormalization04_layer')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', name='Conv05_layer')(x)\n",
    "x = Dropout(0.05, name='Dropout02')(x)\n",
    "x = BatchNormalization(name='BatchNormalization05_layer')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), name='MaxPool02_layer')(x)\n",
    "\n",
    "# fully connected dense block\n",
    "x = Flatten(name='Flatten_layer')(x)\n",
    "x = Dense(512, activation='relu', name='Dense01_layer')(x)\n",
    "x = BatchNormalization(name='BatchNormalization06_layer')(x)\n",
    "x = Dense(num_class, name='logits_layer')(x)\n",
    "x = Dropout(0.1, name='Dropout01')(x)\n",
    "output = Activation('softmax', name='Softmax_layer')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_layer (InputLayer)     (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv01_layer (Conv2D)        (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "Dropout06 (Dropout)          (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "BatchNormalization01_layer ( (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "Conv02_layer (Conv2D)        (None, 26, 26, 32)        9248      \n",
      "_________________________________________________________________\n",
      "Dropout05 (Dropout)          (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "BatchNormalization02_layer ( (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "Conv03_layer (Conv2D)        (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "Dropout04 (Dropout)          (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "BatchNormalization03_layer ( (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "MaxPool01_layer (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv04_layer (Conv2D)        (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "Dropout03 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "BatchNormalization04_layer ( (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "Conv05_layer (Conv2D)        (None, 10, 10, 64)        36928     \n",
      "_________________________________________________________________\n",
      "Dropout02 (Dropout)          (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "BatchNormalization05_layer ( (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "MaxPool02_layer (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "Flatten_layer (Flatten)      (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "Dense01_layer (Dense)        (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "BatchNormalization06_layer ( (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "logits_layer (Dense)         (None, 100)               51300     \n",
      "_________________________________________________________________\n",
      "Dropout01 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "Softmax_layer (Activation)   (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 948,196\n",
      "Trainable params: 946,724\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(input_layer, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "400/400 [==============================] - 41s - loss: 2.9684 - acc: 0.3140 - val_loss: 3.9346 - val_acc: 0.2387\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 40s - loss: 2.1998 - acc: 0.4704 - val_loss: 2.1232 - val_acc: 0.4639\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 39s - loss: 1.9651 - acc: 0.5236 - val_loss: 1.7432 - val_acc: 0.5542\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 39s - loss: 1.8192 - acc: 0.5581 - val_loss: 2.9539 - val_acc: 0.4873\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 40s - loss: 1.7366 - acc: 0.5774 - val_loss: 1.6049 - val_acc: 0.6023\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 40s - loss: 1.6722 - acc: 0.5904 - val_loss: 1.6182 - val_acc: 0.5834\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 40s - loss: 1.6210 - acc: 0.6020 - val_loss: 1.3824 - val_acc: 0.6421\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 39s - loss: 1.5711 - acc: 0.6129 - val_loss: 1.4053 - val_acc: 0.6384\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 40s - loss: 1.5410 - acc: 0.6208 - val_loss: 1.3185 - val_acc: 0.6595\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 40s - loss: 1.5186 - acc: 0.6243 - val_loss: 1.3999 - val_acc: 0.6343\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 40s - loss: 1.4855 - acc: 0.6320 - val_loss: 1.2416 - val_acc: 0.6823\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 38s - loss: 1.4585 - acc: 0.6391 - val_loss: 1.2594 - val_acc: 0.6731\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 40s - loss: 1.4358 - acc: 0.6436 - val_loss: 1.2116 - val_acc: 0.6892\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 40s - loss: 1.4196 - acc: 0.6473 - val_loss: 1.1740 - val_acc: 0.6967\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 40s - loss: 1.4047 - acc: 0.6503 - val_loss: 1.2292 - val_acc: 0.6846\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 39s - loss: 1.3872 - acc: 0.6531 - val_loss: 1.1423 - val_acc: 0.7035\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 40s - loss: 1.3663 - acc: 0.6582 - val_loss: 1.1407 - val_acc: 0.7069\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 39s - loss: 1.3574 - acc: 0.6611 - val_loss: 1.1930 - val_acc: 0.6932\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 39s - loss: 1.3425 - acc: 0.6631 - val_loss: 1.2067 - val_acc: 0.6900\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 39s - loss: 1.3266 - acc: 0.6679 - val_loss: 1.1210 - val_acc: 0.7118\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 39s - loss: 1.3192 - acc: 0.6703 - val_loss: 1.1226 - val_acc: 0.7128\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 40s - loss: 1.3133 - acc: 0.6693 - val_loss: 1.1000 - val_acc: 0.7175\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 39s - loss: 1.3033 - acc: 0.6727 - val_loss: 1.1140 - val_acc: 0.7163\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 39s - loss: 1.2907 - acc: 0.6751 - val_loss: 1.0910 - val_acc: 0.7198\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 39s - loss: 1.2800 - acc: 0.6777 - val_loss: 1.0901 - val_acc: 0.7161\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 39s - loss: 1.2771 - acc: 0.6788 - val_loss: 1.0602 - val_acc: 0.7244\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 40s - loss: 1.2646 - acc: 0.6817 - val_loss: 1.0899 - val_acc: 0.7251\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 40s - loss: 1.2572 - acc: 0.6838 - val_loss: 1.0876 - val_acc: 0.7194\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 39s - loss: 1.2564 - acc: 0.6832 - val_loss: 1.0793 - val_acc: 0.7249\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 39s - loss: 1.2427 - acc: 0.6850 - val_loss: 1.0858 - val_acc: 0.7213\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 40s - loss: 1.2378 - acc: 0.6857 - val_loss: 1.0822 - val_acc: 0.7235\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 39s - loss: 1.2296 - acc: 0.6881 - val_loss: 1.0430 - val_acc: 0.7309\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 39s - loss: 1.2262 - acc: 0.6888 - val_loss: 1.0421 - val_acc: 0.7339\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 39s - loss: 1.2204 - acc: 0.6890 - val_loss: 1.0378 - val_acc: 0.7336\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 39s - loss: 1.2148 - acc: 0.6921 - val_loss: 1.0319 - val_acc: 0.7375\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 40s - loss: 1.2117 - acc: 0.6914 - val_loss: 1.0321 - val_acc: 0.7328\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 39s - loss: 1.2046 - acc: 0.6931 - val_loss: 1.0178 - val_acc: 0.7402\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 39s - loss: 1.1938 - acc: 0.6942 - val_loss: 1.0056 - val_acc: 0.7399\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 39s - loss: 1.1988 - acc: 0.6939 - val_loss: 1.0353 - val_acc: 0.7344\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 39s - loss: 1.1861 - acc: 0.6964 - val_loss: 1.0163 - val_acc: 0.7382\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 39s - loss: 1.1846 - acc: 0.6964 - val_loss: 0.9915 - val_acc: 0.7457\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 40s - loss: 1.1870 - acc: 0.6960 - val_loss: 0.9980 - val_acc: 0.7432\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 39s - loss: 1.1826 - acc: 0.6965 - val_loss: 1.0196 - val_acc: 0.7357\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 39s - loss: 1.1772 - acc: 0.6969 - val_loss: 1.0036 - val_acc: 0.7424\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 39s - loss: 1.1671 - acc: 0.7012 - val_loss: 1.0045 - val_acc: 0.7431\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 39s - loss: 1.1691 - acc: 0.6992 - val_loss: 1.0009 - val_acc: 0.7450\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 39s - loss: 1.1625 - acc: 0.7017 - val_loss: 1.0103 - val_acc: 0.7422\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 40s - loss: 1.1563 - acc: 0.7037 - val_loss: 1.0121 - val_acc: 0.7427\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 40s - loss: 1.1531 - acc: 0.7040 - val_loss: 1.0188 - val_acc: 0.7395\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 39s - loss: 1.1524 - acc: 0.7037 - val_loss: 1.0099 - val_acc: 0.7418\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 39s - loss: 1.1530 - acc: 0.7031 - val_loss: 0.9692 - val_acc: 0.7508\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 40s - loss: 1.1442 - acc: 0.7064 - val_loss: 1.0155 - val_acc: 0.7376\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 39s - loss: 1.1418 - acc: 0.7068 - val_loss: 1.0064 - val_acc: 0.7434\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 40s - loss: 1.1426 - acc: 0.7060 - val_loss: 0.9883 - val_acc: 0.7482\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 39s - loss: 1.1316 - acc: 0.7075 - val_loss: 0.9889 - val_acc: 0.7445\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 39s - loss: 1.1322 - acc: 0.7092 - val_loss: 0.9824 - val_acc: 0.7494\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 39s - loss: 1.1262 - acc: 0.7106 - val_loss: 0.9933 - val_acc: 0.7455\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 39s - loss: 1.1281 - acc: 0.7081 - val_loss: 0.9899 - val_acc: 0.7463\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 39s - loss: 1.1175 - acc: 0.7108 - val_loss: 0.9863 - val_acc: 0.7450\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 39s - loss: 1.1204 - acc: 0.7115 - val_loss: 0.9927 - val_acc: 0.7472\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 39s - loss: 1.1153 - acc: 0.7105 - val_loss: 0.9938 - val_acc: 0.7455\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 40s - loss: 1.1136 - acc: 0.7103 - val_loss: 0.9729 - val_acc: 0.7518\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 40s - loss: 1.1140 - acc: 0.7112 - val_loss: 0.9863 - val_acc: 0.7496\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 39s - loss: 1.1094 - acc: 0.7127 - val_loss: 0.9704 - val_acc: 0.7512\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 39s - loss: 1.1120 - acc: 0.7124 - val_loss: 0.9839 - val_acc: 0.7484\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 39s - loss: 1.1094 - acc: 0.7116 - val_loss: 0.9740 - val_acc: 0.7525\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 39s - loss: 1.1034 - acc: 0.7136 - val_loss: 0.9557 - val_acc: 0.7550\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 39s - loss: 1.0967 - acc: 0.7145 - val_loss: 0.9750 - val_acc: 0.7512\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 39s - loss: 1.0998 - acc: 0.7133 - val_loss: 0.9744 - val_acc: 0.7531\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 39s - loss: 1.1036 - acc: 0.7125 - val_loss: 0.9703 - val_acc: 0.7568\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 39s - loss: 1.0974 - acc: 0.7151 - val_loss: 0.9988 - val_acc: 0.7476\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 39s - loss: 1.0941 - acc: 0.7163 - val_loss: 0.9810 - val_acc: 0.7477\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 39s - loss: 1.0976 - acc: 0.7139 - val_loss: 0.9844 - val_acc: 0.7525\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 39s - loss: 1.0851 - acc: 0.7176 - val_loss: 0.9737 - val_acc: 0.7530\n",
      "Epoch 75/100\n",
      "400/400 [==============================] - 39s - loss: 1.0869 - acc: 0.7170 - val_loss: 0.9735 - val_acc: 0.7544\n",
      "Epoch 76/100\n",
      "400/400 [==============================] - 39s - loss: 1.0844 - acc: 0.7182 - val_loss: 0.9775 - val_acc: 0.7473\n",
      "Epoch 77/100\n",
      "400/400 [==============================] - 39s - loss: 1.0787 - acc: 0.7194 - val_loss: 1.0013 - val_acc: 0.7438\n",
      "Epoch 78/100\n",
      "400/400 [==============================] - 39s - loss: 1.0831 - acc: 0.7192 - val_loss: 0.9752 - val_acc: 0.7529\n",
      "Epoch 79/100\n",
      "400/400 [==============================] - 39s - loss: 1.0811 - acc: 0.7172 - val_loss: 0.9610 - val_acc: 0.7550\n",
      "Epoch 80/100\n",
      "400/400 [==============================] - 38s - loss: 1.0751 - acc: 0.7189 - val_loss: 0.9725 - val_acc: 0.7509\n",
      "Epoch 81/100\n",
      "400/400 [==============================] - 39s - loss: 1.0740 - acc: 0.7198 - val_loss: 0.9654 - val_acc: 0.7549\n",
      "Epoch 82/100\n",
      "400/400 [==============================] - 39s - loss: 1.0777 - acc: 0.7189 - val_loss: 0.9612 - val_acc: 0.7523\n",
      "Epoch 83/100\n",
      "400/400 [==============================] - 39s - loss: 1.0685 - acc: 0.7213 - val_loss: 0.9856 - val_acc: 0.7491\n",
      "Epoch 84/100\n",
      "400/400 [==============================] - 39s - loss: 1.0717 - acc: 0.7201 - val_loss: 0.9758 - val_acc: 0.7518\n",
      "Epoch 85/100\n",
      "400/400 [==============================] - 39s - loss: 1.0662 - acc: 0.7213 - val_loss: 0.9598 - val_acc: 0.7571\n",
      "Epoch 86/100\n",
      "400/400 [==============================] - 39s - loss: 1.0619 - acc: 0.7216 - val_loss: 0.9639 - val_acc: 0.7556\n",
      "Epoch 87/100\n",
      "400/400 [==============================] - 39s - loss: 1.0652 - acc: 0.7219 - val_loss: 0.9582 - val_acc: 0.7559\n",
      "Epoch 88/100\n",
      "400/400 [==============================] - 39s - loss: 1.0601 - acc: 0.7232 - val_loss: 0.9571 - val_acc: 0.7552\n",
      "Epoch 89/100\n",
      "400/400 [==============================] - 39s - loss: 1.0587 - acc: 0.7239 - val_loss: 0.9548 - val_acc: 0.7575\n",
      "Epoch 90/100\n",
      "400/400 [==============================] - 39s - loss: 1.0583 - acc: 0.7239 - val_loss: 0.9792 - val_acc: 0.7532\n",
      "Epoch 91/100\n",
      "400/400 [==============================] - 39s - loss: 1.0530 - acc: 0.7244 - val_loss: 0.9684 - val_acc: 0.7542\n",
      "Epoch 92/100\n",
      "400/400 [==============================] - 39s - loss: 1.0596 - acc: 0.7203 - val_loss: 0.9646 - val_acc: 0.7540\n",
      "Epoch 93/100\n",
      "400/400 [==============================] - 39s - loss: 1.0577 - acc: 0.7228 - val_loss: 0.9676 - val_acc: 0.7586\n",
      "Epoch 94/100\n",
      "400/400 [==============================] - 39s - loss: 1.0579 - acc: 0.7224 - val_loss: 0.9666 - val_acc: 0.7558\n",
      "Epoch 95/100\n",
      "400/400 [==============================] - 39s - loss: 1.0538 - acc: 0.7228 - val_loss: 0.9662 - val_acc: 0.7531\n",
      "Epoch 96/100\n",
      "400/400 [==============================] - 39s - loss: 1.0478 - acc: 0.7230 - val_loss: 0.9559 - val_acc: 0.7559\n",
      "Epoch 97/100\n",
      "400/400 [==============================] - 39s - loss: 1.0500 - acc: 0.7252 - val_loss: 0.9771 - val_acc: 0.7539\n",
      "Epoch 98/100\n",
      "400/400 [==============================] - 39s - loss: 1.0440 - acc: 0.7277 - val_loss: 0.9621 - val_acc: 0.7546\n",
      "Epoch 99/100\n",
      "400/400 [==============================] - 39s - loss: 1.0479 - acc: 0.7244 - val_loss: 0.9536 - val_acc: 0.7589\n",
      "Epoch 100/100\n",
      "400/400 [==============================] - 39s - loss: 1.0465 - acc: 0.7244 - val_loss: 0.9632 - val_acc: 0.7566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb10c159950>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.03\n",
    "decay = 0.2\n",
    "batch_size = 2**8\n",
    "epochs = 100\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='loss', min_delta=0.001, patience=10),\n",
    "             LearningRateScheduler(lambda epoch:learning_rate/(1 + decay * epoch)),\n",
    "             ReduceLROnPlateau(monitor='loss')]\n",
    "             \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "K.set_value(model.optimizer.lr, learning_rate)\n",
    "model.fit_generator(train_data_gen.flow(train_data, train_target, batch_size=batch_size),\n",
    "                    validation_data=test_data_gen.flow(test_data, test_target, batch_size=batch_size),\n",
    "                    steps_per_epoch=train_data.shape[0] // batch_size,\n",
    "                    validation_steps=test_data.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    workers=8,\n",
    "                    callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
